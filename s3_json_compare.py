#!/usr/bin/env python3
"""
S3 JSON ë°ì´í„° ì¼ì¹˜ì„± ë¹„êµ í”„ë¡œê·¸ë¨

AWS S3 ë²„í‚·ì— ì €ì¥ëœ ëŒ€ìš©ëŸ‰ JSON ë°ì´í„°ì˜ ë°±ì—… ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ëŠ” í”„ë¡œê·¸ë¨
"""

import asyncio
import concurrent.futures
import gzip
import hashlib
import json
import logging
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Generator, List, Optional, Tuple, Union

import boto3
import ijson
from botocore.exceptions import ClientError, NoCredentialsError
from tqdm import tqdm

from utils.s3_handler import S3Handler
from utils.json_processor import JSONProcessor
from utils.report_generator import ReportGenerator
from utils.logger import setup_logger


@dataclass
class CompareResult:
    """ë¹„êµ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    file_path: str
    source_records: int
    backup_records: int
    matched_records: int
    mismatched_records: int
    missing_in_backup: int
    missing_in_source: int
    errors: List[str]
    processing_time: float


class S3JSONComparer:
    """S3 JSON ë°ì´í„° ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, source_bucket: str, backup_bucket: str, 
                 processes: int = 4, chunk_size: int = 10000):
        self.source_bucket = source_bucket
        self.backup_bucket = backup_bucket
        self.processes = processes
        self.chunk_size = chunk_size
        self.logger = setup_logger(__name__)
        
        # S3 í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        self.s3_handler = S3Handler()
        self.json_processor = JSONProcessor(chunk_size)
        self.report_generator = ReportGenerator()
        
        # ê²°ê³¼ ì €ì¥
        self.compare_results: List[CompareResult] = []
        
    def get_file_list(self, bucket: str, prefix: str = "") -> List[str]:
        """S3 ë²„í‚·ì—ì„œ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        try:
            return self.s3_handler.list_files(bucket, prefix)
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def compare_file_pair(self, file_path: str) -> CompareResult:
        """ê°œë³„ íŒŒì¼ ìŒì„ ë¹„êµí•©ë‹ˆë‹¤"""
        start_time = time.time()
        result = CompareResult(
            file_path=file_path,
            source_records=0,
            backup_records=0,
            matched_records=0,
            mismatched_records=0,
            missing_in_backup=0,
            missing_in_source=0,
            errors=[],
            processing_time=0
        )
        
        try:
            # ì†ŒìŠ¤ ë°ì´í„° í•´ì‹œ ìƒì„±
            source_hashes = self._generate_file_hashes(
                self.source_bucket, file_path
            )
            result.source_records = len(source_hashes)
            
            # ë°±ì—… ë°ì´í„° í•´ì‹œ ìƒì„±
            backup_hashes = self._generate_file_hashes(
                self.backup_bucket, file_path
            )
            result.backup_records = len(backup_hashes)
            
            # í•´ì‹œ ë¹„êµ
            source_set = set(source_hashes)
            backup_set = set(backup_hashes)
            
            result.matched_records = len(source_set.intersection(backup_set))
            result.missing_in_backup = len(source_set - backup_set)
            result.missing_in_source = len(backup_set - source_set)
            result.mismatched_records = (
                result.missing_in_backup + result.missing_in_source
            )
            
        except Exception as e:
            error_msg = f"íŒŒì¼ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            result.errors.append(error_msg)
            self.logger.error(error_msg)
        
        result.processing_time = time.time() - start_time
        return result
    
    def _generate_file_hashes(self, bucket: str, file_path: str) -> List[str]:
        """íŒŒì¼ì—ì„œ ë ˆì½”ë“œë³„ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        hashes = []
        
        try:
            # S3ì—ì„œ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
            stream = self.s3_handler.get_file_stream(bucket, file_path)
            
            # ì••ì¶• íŒŒì¼ ì²˜ë¦¬
            if file_path.endswith('.gz'):
                gzip_stream = gzip.GzipFile(fileobj=stream)
                # gzip ìŠ¤íŠ¸ë¦¼ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
                import io
                text_stream = io.TextIOWrapper(gzip_stream, encoding='utf-8')
            else:
                # ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¼ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
                import io
                text_stream = io.TextIOWrapper(stream, encoding='utf-8')
            
            # JSON ì²˜ë¦¬ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            for record in self.json_processor.process_stream(text_stream):
                # ë ˆì½”ë“œë¥¼ ì •ê·œí™”í•˜ê³  í•´ì‹œ ìƒì„±
                normalized_record = self._normalize_record(record)
                record_hash = self._generate_record_hash(normalized_record)
                hashes.append(record_hash)
                
        except Exception as e:
            raise Exception(f"íŒŒì¼ í•´ì‹œ ìƒì„± ì‹¤íŒ¨ ({file_path}): {str(e)}")
        
        return hashes
    
    def _normalize_record(self, record: Dict) -> Dict:
        """ë ˆì½”ë“œë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤ (í‚¤ ìˆœì„œ ì •ë ¬)"""
        if isinstance(record, dict):
            return {k: self._normalize_record(v) for k, v in sorted(record.items())}
        elif isinstance(record, list):
            return [self._normalize_record(item) for item in record]
        else:
            return record
    
    def _generate_record_hash(self, record: Dict) -> str:
        """ë ˆì½”ë“œì˜ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        record_json = json.dumps(record, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(record_json.encode('utf-8')).hexdigest()
    
    def _find_record_by_hash(self, bucket: str, file_path: str, target_hash: str) -> Optional[Dict]:
        """íŠ¹ì • í•´ì‹œì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ JSON ë ˆì½”ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤"""
        try:
            # S3ì—ì„œ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
            stream = self.s3_handler.get_file_stream(bucket, file_path)
            
            # ì••ì¶• íŒŒì¼ ì²˜ë¦¬
            if file_path.endswith('.gz'):
                gzip_stream = gzip.GzipFile(fileobj=stream)
                import io
                text_stream = io.TextIOWrapper(gzip_stream, encoding='utf-8')
            else:
                import io
                text_stream = io.TextIOWrapper(stream, encoding='utf-8')
            
            # JSON ì²˜ë¦¬ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            for record in self.json_processor.process_stream(text_stream):
                # ë ˆì½”ë“œë¥¼ ì •ê·œí™”í•˜ê³  í•´ì‹œ ìƒì„±
                normalized_record = self._normalize_record(record)
                record_hash = self._generate_record_hash(normalized_record)
                
                if record_hash == target_hash:
                    return record  # ì›ë³¸ ë ˆì½”ë“œ ë°˜í™˜ (ì •ê·œí™”ë˜ì§€ ì•Šì€)
                    
        except Exception as e:
            self.logger.error(f"í•´ì‹œë¡œ ë ˆì½”ë“œ ì°¾ê¸° ì‹¤íŒ¨ ({file_path}, {target_hash[:16]}...): {str(e)}")
        
        return None
    
    def compare_buckets(self, source_prefix: str = "", backup_prefix: str = "",
                       report_path: str = "compare_report.csv") -> bool:
        """ë‘ ë²„í‚·ì˜ ëª¨ë“  íŒŒì¼ì„ ë¹„êµí•©ë‹ˆë‹¤ - SQLite in-memory (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)"""
        import sqlite3
        import tempfile
        
        self.logger.info("S3 ë²„í‚· ë¹„êµ ì‹œì‘ (SQLite in-memory + ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)")
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        source_files = self.get_file_list(self.source_bucket, source_prefix)
        backup_files = self.get_file_list(self.backup_bucket, backup_prefix)
        
        self.logger.info(f"ì†ŒìŠ¤ ë²„í‚· íŒŒì¼ ìˆ˜: {len(source_files)}")
        self.logger.info(f"ë°±ì—… ë²„í‚· íŒŒì¼ ìˆ˜: {len(backup_files)}")
        
        # SQLite in-memory ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        conn = sqlite3.connect(':memory:')
        cursor = conn.cursor()
        
        # í•´ì‹œ í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE source_hashes (
                hash TEXT PRIMARY KEY,
                file_path TEXT,
                record_count INTEGER
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE backup_hashes (
                hash TEXT PRIMARY KEY,
                file_path TEXT,
                record_count INTEGER
            )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ í–¥ìƒ)
        cursor.execute('CREATE INDEX idx_source_hash ON source_hashes(hash)')
        cursor.execute('CREATE INDEX idx_backup_hash ON backup_hashes(hash)')
        
        # ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)
        self.logger.info("ì†ŒìŠ¤ ë²„í‚· íŒŒì¼ ë‚´ìš© í•´ì‹œí™” ì‹œì‘...")
        source_total_records = 0
        
        with tqdm(total=len(source_files), desc="ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬") as pbar:
            for file_path in source_files:
                try:
                    file_hashes = self._generate_file_hashes(self.source_bucket, file_path)
                    source_total_records += len(file_hashes)
                    
                    # SQLiteì— ë°°ì¹˜ ì‚½ì…
                    if file_hashes:
                        hash_data = [(hash_val, file_path, 1) for hash_val in file_hashes]
                        cursor.executemany(
                            'INSERT OR IGNORE INTO source_hashes (hash, file_path, record_count) VALUES (?, ?, ?)',
                            hash_data
                        )
                    
                    pbar.set_postfix({'current': file_path, 'records': len(file_hashes)})
                    
                except Exception as e:
                    self.logger.error(f"ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
                    result = CompareResult(
                        file_path=file_path,
                        source_records=0,
                        backup_records=0,
                        matched_records=0,
                        mismatched_records=0,
                        missing_in_backup=0,
                        missing_in_source=0,
                        errors=[f"ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"],
                        processing_time=0
                    )
                    self.compare_results.append(result)
                pbar.update(1)
        
        # ë°±ì—… íŒŒì¼ ì²˜ë¦¬ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)
        self.logger.info("ë°±ì—… ë²„í‚· íŒŒì¼ ë‚´ìš© í•´ì‹œí™” ì‹œì‘...")
        backup_total_records = 0
        
        with tqdm(total=len(backup_files), desc="ë°±ì—… íŒŒì¼ ì²˜ë¦¬") as pbar:
            for file_path in backup_files:
                try:
                    file_hashes = self._generate_file_hashes(self.backup_bucket, file_path)
                    backup_total_records += len(file_hashes)
                    
                    # SQLiteì— ë°°ì¹˜ ì‚½ì…
                    if file_hashes:
                        hash_data = [(hash_val, file_path, 1) for hash_val in file_hashes]
                        cursor.executemany(
                            'INSERT OR IGNORE INTO backup_hashes (hash, file_path, record_count) VALUES (?, ?, ?)',
                            hash_data
                        )
                    
                    pbar.set_postfix({'current': file_path, 'records': len(file_hashes)})
                    
                except Exception as e:
                    self.logger.error(f"ë°±ì—… íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
                    result = CompareResult(
                        file_path=file_path,
                        source_records=0,
                        backup_records=0,
                        matched_records=0,
                        mismatched_records=0,
                        missing_in_backup=0,
                        missing_in_source=0,
                        errors=[f"ë°±ì—… íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"],
                        processing_time=0
                    )
                    self.compare_results.append(result)
                pbar.update(1)
        
        # SQLì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ë¹„êµ
        self.logger.info("SQLiteë¥¼ ì‚¬ìš©í•œ ì „ì²´ ë‚´ìš© ë¹„êµ ì‹œì‘...")
        
        # ì¼ì¹˜í•˜ëŠ” í•´ì‹œ ìˆ˜
        cursor.execute('''
            SELECT COUNT(*) FROM source_hashes s
            INNER JOIN backup_hashes b ON s.hash = b.hash
        ''')
        matched_records = cursor.fetchone()[0]
        
        # ì†ŒìŠ¤ì—ì„œë§Œ ìˆëŠ” í•´ì‹œ ìˆ˜
        cursor.execute('''
            SELECT COUNT(*) FROM source_hashes s
            LEFT JOIN backup_hashes b ON s.hash = b.hash
            WHERE b.hash IS NULL
        ''')
        missing_in_backup = cursor.fetchone()[0]
        
        # ë°±ì—…ì—ì„œë§Œ ìˆëŠ” í•´ì‹œ ìˆ˜
        cursor.execute('''
            SELECT COUNT(*) FROM backup_hashes b
            LEFT JOIN source_hashes s ON b.hash = s.hash
            WHERE s.hash IS NULL
        ''')
        missing_in_source = cursor.fetchone()[0]
        
        # ë¶ˆì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ ì •ë³´ ìˆ˜ì§‘
        mismatched_records = []
        
        if missing_in_backup > 0 or missing_in_source > 0:
            self.logger.info("ë¶ˆì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œì˜ JSON ë‚´ì—­ì„ ì¶œë ¥í•©ë‹ˆë‹¤...")
            
            # ì†ŒìŠ¤ì—ì„œë§Œ ìˆëŠ” ë ˆì½”ë“œ ìƒ˜í”Œ ì¶œë ¥ (ìµœëŒ€ 10ê°œ)
            if missing_in_backup > 0:
                cursor.execute('''
                    SELECT s.hash, s.file_path FROM source_hashes s
                    LEFT JOIN backup_hashes b ON s.hash = b.hash
                    WHERE b.hash IS NULL
                    LIMIT 10
                ''')
                source_only_hashes = cursor.fetchall()
                
                self.logger.info(f"ì†ŒìŠ¤ì—ì„œë§Œ ìˆëŠ” ë ˆì½”ë“œ ìƒ˜í”Œ ({len(source_only_hashes)}ê°œ):")
                for hash_val, file_path in source_only_hashes:
                    # í•´ë‹¹ í•´ì‹œì˜ ì›ë³¸ JSON ë ˆì½”ë“œ ì°¾ê¸°
                    original_record = self._find_record_by_hash(self.source_bucket, file_path, hash_val)
                    if original_record:
                        self.logger.info(f"  í•´ì‹œ: {hash_val[:16]}...")
                        self.logger.info(f"  íŒŒì¼: {file_path}")
                        self.logger.info(f"  JSON: {json.dumps(original_record, ensure_ascii=False, indent=2)}")
                        self.logger.info("  " + "-" * 50)
                        
                        # ë¶ˆì¼ì¹˜ ë ˆì½”ë“œ ì •ë³´ ìˆ˜ì§‘
                        mismatched_records.append({
                            'hash': hash_val,
                            'file_path': file_path,
                            'bucket_type': 'source_only',
                            'json_content': json.dumps(original_record, ensure_ascii=False),
                            'hash_short': hash_val[:16] + '...'
                        })
            
            # ë°±ì—…ì—ì„œë§Œ ìˆëŠ” ë ˆì½”ë“œ ìƒ˜í”Œ ì¶œë ¥ (ìµœëŒ€ 10ê°œ)
            if missing_in_source > 0:
                cursor.execute('''
                    SELECT b.hash, b.file_path FROM backup_hashes b
                    LEFT JOIN source_hashes s ON b.hash = s.hash
                    WHERE s.hash IS NULL
                    LIMIT 10
                ''')
                backup_only_hashes = cursor.fetchall()
                
                self.logger.info(f"ë°±ì—…ì—ì„œë§Œ ìˆëŠ” ë ˆì½”ë“œ ìƒ˜í”Œ ({len(backup_only_hashes)}ê°œ):")
                for hash_val, file_path in backup_only_hashes:
                    # í•´ë‹¹ í•´ì‹œì˜ ì›ë³¸ JSON ë ˆì½”ë“œ ì°¾ê¸°
                    original_record = self._find_record_by_hash(self.backup_bucket, file_path, hash_val)
                    if original_record:
                        self.logger.info(f"  í•´ì‹œ: {hash_val[:16]}...")
                        self.logger.info(f"  íŒŒì¼: {file_path}")
                        self.logger.info(f"  JSON: {json.dumps(original_record, ensure_ascii=False, indent=2)}")
                        self.logger.info("  " + "-" * 50)
                        
                        # ë¶ˆì¼ì¹˜ ë ˆì½”ë“œ ì •ë³´ ìˆ˜ì§‘
                        mismatched_records.append({
                            'hash': hash_val,
                            'file_path': file_path,
                            'bucket_type': 'backup_only',
                            'json_content': json.dumps(original_record, ensure_ascii=False),
                            'hash_short': hash_val[:16] + '...'
                        })
        
        total_mismatched_records = missing_in_backup + missing_in_source
        
        self.logger.info(f"ì†ŒìŠ¤ ì´ ë ˆì½”ë“œ: {source_total_records}")
        self.logger.info(f"ë°±ì—… ì´ ë ˆì½”ë“œ: {backup_total_records}")
        self.logger.info(f"ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ: {matched_records}")
        self.logger.info(f"ë¶ˆì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ: {total_mismatched_records}")
        self.logger.info(f"  - ì†ŒìŠ¤ì—ì„œë§Œ: {missing_in_backup}")
        self.logger.info(f"  - ë°±ì—…ì—ì„œë§Œ: {missing_in_source}")
        
        # ì „ì²´ ë¹„êµ ê²°ê³¼ ìƒì„±
        overall_result = CompareResult(
            file_path="OVERALL_COMPARISON",
            source_records=source_total_records,
            backup_records=backup_total_records,
            matched_records=matched_records,
            mismatched_records=total_mismatched_records,
            missing_in_backup=missing_in_backup,
            missing_in_source=missing_in_source,
            errors=[],
            processing_time=0
        )
        self.compare_results.append(overall_result)
        
        # ê°œë³„ íŒŒì¼ í†µê³„ (ìƒ˜í”Œë§)
        cursor.execute('''
            SELECT file_path, COUNT(*) as record_count 
            FROM source_hashes 
            GROUP BY file_path 
            ORDER BY record_count DESC 
            LIMIT 10
        ''')
        source_file_stats = cursor.fetchall()
        
        for file_path, record_count in source_file_stats:
            result = CompareResult(
                file_path=f"SOURCE: {file_path}",
                source_records=record_count,
                backup_records=0,
                matched_records=0,
                mismatched_records=0,
                missing_in_backup=0,
                missing_in_source=0,
                errors=[],
                processing_time=0
            )
            self.compare_results.append(result)
        
        cursor.execute('''
            SELECT file_path, COUNT(*) as record_count 
            FROM backup_hashes 
            GROUP BY file_path 
            ORDER BY record_count DESC 
            LIMIT 10
        ''')
        backup_file_stats = cursor.fetchall()
        
        for file_path, record_count in backup_file_stats:
            result = CompareResult(
                file_path=f"BACKUP: {file_path}",
                source_records=0,
                backup_records=record_count,
                matched_records=0,
                mismatched_records=0,
                missing_in_backup=0,
                missing_in_source=0,
                errors=[],
                processing_time=0
            )
            self.compare_results.append(result)
        
        # ì—°ê²° ì¢…ë£Œ
        conn.close()
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        self.report_generator.generate_report(self.compare_results, report_path)
        
        # ë¶ˆì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        if mismatched_records:
            detailed_report_path = report_path.replace('.csv', '_detailed.csv')
            self.report_generator.generate_detailed_mismatch_report_with_json(
                mismatched_records, detailed_report_path
            )
        
        # ê²°ê³¼ ìš”ì•½
        self.logger.info(f"ë¹„êµ ì™„ë£Œ: ì†ŒìŠ¤ {len(source_files)}ê°œ, ë°±ì—… {len(backup_files)}ê°œ íŒŒì¼")
        self.logger.info(f"ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ: {matched_records}")
        self.logger.info(f"ë¶ˆì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ: {total_mismatched_records}")
        
        return total_mismatched_records == 0


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë©”ë‰´ í˜•íƒœ"""
    print("=" * 60)
    print("ğŸš€ S3 JSON ë°ì´í„° ì¼ì¹˜ì„± ë¹„êµ í”„ë¡œê·¸ë¨")
    print("=" * 60)
    
    # S3 URL íŒŒì‹± í•¨ìˆ˜
    def parse_s3_url(url: str) -> Tuple[str, str]:
        """S3 URLì„ íŒŒì‹±í•©ë‹ˆë‹¤"""
        if not url.startswith("s3://"):
            raise ValueError("S3 URLì€ s3://ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤")
        
        url = url[5:]  # s3:// ì œê±°
        parts = url.split("/", 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ""
        
        return bucket, prefix
    
    try:
        # ë©”ë‰´ ì…ë ¥ ë°›ê¸°
        print("\nğŸ“‹ ì„¤ì • ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:")
        
        # ì†ŒìŠ¤ ë²„í‚· ì…ë ¥
        while True:
            source_bucket_url = input("\nğŸ” ì†ŒìŠ¤ S3 ë²„í‚· URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: s3://my-bucket/path/): ").strip()
            if source_bucket_url:
                try:
                    source_bucket, source_prefix = parse_s3_url(source_bucket_url)
                    print(f"âœ… ì†ŒìŠ¤ ë²„í‚·: {source_bucket}")
                    print(f"âœ… ì†ŒìŠ¤ ê²½ë¡œ: {source_prefix if source_prefix else '(ë£¨íŠ¸)'}")
                    break
                except ValueError as e:
                    print(f"âŒ ì˜¤ë¥˜: {e}")
            else:
                print("âŒ ì†ŒìŠ¤ ë²„í‚· URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ë°±ì—… ë²„í‚· ì…ë ¥
        while True:
            backup_bucket_url = input("\nğŸ’¾ ë°±ì—… S3 ë²„í‚· URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: s3://my-backup-bucket/path/): ").strip()
            if backup_bucket_url:
                try:
                    backup_bucket, backup_prefix = parse_s3_url(backup_bucket_url)
                    print(f"âœ… ë°±ì—… ë²„í‚·: {backup_bucket}")
                    print(f"âœ… ë°±ì—… ê²½ë¡œ: {backup_prefix if backup_prefix else '(ë£¨íŠ¸)'}")
                    break
                except ValueError as e:
                    print(f"âŒ ì˜¤ë¥˜: {e}")
            else:
                print("âŒ ë°±ì—… ë²„í‚· URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ë¹„êµ ëª¨ë“œ ì„ íƒ
        print("\nğŸ“Š ë¹„êµ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. JSONL (JSON Lines) - í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON ê°ì²´")
        print("2. Array - JSON ë°°ì—´ í˜•íƒœ")
        print("3. CSV - CSV í˜•íƒœ")
        
        while True:
            mode_choice = input("ì„ íƒ (1-3, ê¸°ë³¸ê°’: 1): ").strip()
            if not mode_choice:
                compare_mode = "jsonl"
                break
            elif mode_choice in ["1", "2", "3"]:
                mode_map = {"1": "jsonl", "2": "array", "3": "csv"}
                compare_mode = mode_map[mode_choice]
                break
            else:
                print("âŒ 1, 2, 3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        print(f"âœ… ë¹„êµ ëª¨ë“œ: {compare_mode}")
        
        # ì²­í¬ í¬ê¸° ì…ë ¥
        while True:
            chunk_size_input = input("\nğŸ“¦ ì²­í¬ í¬ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 20000): ").strip()
            if not chunk_size_input:
                chunk_size = 20000
                break
            try:
                chunk_size = int(chunk_size_input)
                if chunk_size > 0:
                    break
                else:
                    print("âŒ ì²­í¬ í¬ê¸°ëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        print(f"âœ… ì²­í¬ í¬ê¸°: {chunk_size}")
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ ì…ë ¥
        report_path = input("\nğŸ“„ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: ./detailed_report.csv): ").strip()
        if not report_path:
            report_path = "./detailed_report.csv"
        
        print(f"âœ… ë¦¬í¬íŠ¸ ê²½ë¡œ: {report_path}")
        
        # ë¡œê·¸ ë ˆë²¨ ì„ íƒ
        print("\nğŸ“ ë¡œê·¸ ë ˆë²¨ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. DEBUG - ìƒì„¸í•œ ë””ë²„ê·¸ ì •ë³´")
        print("2. INFO - ì¼ë°˜ ì •ë³´ (ê¸°ë³¸ê°’)")
        print("3. WARNING - ê²½ê³ ë§Œ")
        print("4. ERROR - ì˜¤ë¥˜ë§Œ")
        
        while True:
            log_choice = input("ì„ íƒ (1-4, ê¸°ë³¸ê°’: 2): ").strip()
            if not log_choice:
                log_level = "INFO"
                break
            elif log_choice in ["1", "2", "3", "4"]:
                level_map = {"1": "DEBUG", "2": "INFO", "3": "WARNING", "4": "ERROR"}
                log_level = level_map[log_choice]
                break
            else:
                print("âŒ 1, 2, 3, 4 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        print(f"âœ… ë¡œê·¸ ë ˆë²¨: {log_level}")
        
        # ì„¤ì • í™•ì¸
        print("\n" + "=" * 60)
        print("ğŸ“‹ ì…ë ¥ëœ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print(f"ğŸ” ì†ŒìŠ¤: {source_bucket_url}")
        print(f"ğŸ’¾ ë°±ì—…: {backup_bucket_url}")
        print(f"ğŸ“Š ëª¨ë“œ: {compare_mode}")
        print(f"ğŸ“¦ ì²­í¬: {chunk_size}")
        print(f"ğŸ“„ ë¦¬í¬íŠ¸: {report_path}")
        print(f"ğŸ“ ë¡œê·¸: {log_level}")
        print("=" * 60)
        
        confirm = input("\nğŸš€ ë¹„êµë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes']:
            print("âŒ ë¹„êµê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)
        
        # ë¡œê·¸ ì„¤ì •
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        print("\nğŸš€ S3 JSON ë°ì´í„° ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ë¹„êµ í”„ë¡œê·¸ë¨ ì´ˆê¸°í™”
        comparer = S3JSONComparer(
            source_bucket=source_bucket,
            backup_bucket=backup_bucket,
            processes=1,  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©
            chunk_size=chunk_size
        )
        
        # ë¹„êµ ì‹¤í–‰
        success = comparer.compare_buckets(
            source_prefix=source_prefix,
            backup_prefix=backup_prefix,
            report_path=report_path
        )
        
        print("\n" + "=" * 60)
        if success:
            print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤!")
            sys.exit(0)
        else:
            print("âŒ ë°ì´í„° ë¶ˆì¼ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 